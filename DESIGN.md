# Design Document: Financial Q&A RAG Agent

This document outlines the design choices for the RAG-based financial Q&A system as per the assignment requirements.

## 1. Data Acquisition Strategy

[cite_start]An automated downloader was implemented to fetch 10-K filings directly from the SEC EDGAR database[cite: 14].
 [cite_start]This approach was chosen over manual download to demonstrate data engineering skills and create a more robust, end-to-end pipeline, securing bonus points[cite: 99].

-   **Mechanism**: The script uses the `requests` library to query the SEC's `browse-edgar` endpoint to find filing accession numbers. It then constructs the final URL to the primary HTML document (`-index.htm`).
-   **Parsing**: `BeautifulSoup` is used to parse the HTML and extract the text content from the `document` tag, which contains the core filing text.
-   **Caching**: To speed up subsequent runs, downloaded text content is cached locally in a `data` directory.

## 2. RAG Pipeline Architecture

### [cite_start]Chunking Strategy [cite: 108]

-   **Method**: I used the `RecursiveCharacterTextSplitter` from the LangChain library.
-   **Parameters**: `chunk_size=1000` characters and `chunk_overlap=200` characters.
-   **Justification**: This strategy is effective for semi-structured text like 10-K filings. It attempts to split text based on semantic boundaries (paragraphs, newlines) before resorting to a hard character cut. The 1000-character size provides enough context for the embedding model without being too large, and the 200-character overlap ensures that semantic context is not lost at the boundaries between chunks.

### [cite_start]Embedding Model [cite: 109]

-   **Model**: `all-MiniLM-L6-v2` from the `sentence-transformers` library.
-   **Justification**:
    -   **Performance**: This is a well-regarded, lightweight model that provides a strong balance between speed and embedding quality for semantic search tasks.
    -   [cite_start]**Cost-Effective**: It's an open-source model that runs locally, eliminating API costs and dependencies on services like OpenAI or Cohere[cite: 23].
    -   **Simplicity**: It's easy to integrate and does not require complex setup or API key management for the embedding part of the pipeline.

### Vector Store

-   [cite_start]**Technology**: FAISS (Facebook AI Similarity Search)[cite: 89].
-   **Justification**: FAISS is an extremely efficient, in-memory library for vector similarity search. For the scale of this project (9 documents), it provides near-instantaneous retrieval times without the overhead of setting up a database like ChromaDB. The entire index and document chunks are cached to disk using `pickle` for fast re-loading.

## [cite_start]3. Agent and Query Decomposition Approach [cite: 110]

[cite_start]The agent's logic is the core of this system, designed to handle both simple and complex comparative queries[cite: 5].

-   **Architecture**: The agent uses a **Plan-and-Execute** strategy driven by an LLM (Google's Gemini Pro).
    1.  **Planner**: The first LLM call acts as a "planner." It receives the user's query and a carefully crafted prompt instructing it to break the query down into simple, atomic sub-queries. It is instructed to return a JSON list of these sub-queries. [cite_start]This directly addresses the query decomposition requirement[cite: 27].
    2.  **Executor**: For each sub-query generated by the planner:
        a. The sub-query is used to retrieve the Top-K (k=5) most relevant text chunks from the FAISS vector store.
        b. A second LLM call is made, providing the sub-query and the retrieved context. This LLM is prompted to act as an "extractive QA model," finding the specific answer and the supporting text excerpt from the context.
    3.  **Synthesizer**: A final, third LLM call acts as the "synthesizer." It receives the original user query along with all the answers and sources gathered by the executor. [cite_start]Its prompt is to combine this information into a single, coherent final answer and generate the reasoning, following the required JSON output format[cite: 28, 52].

-   **Justification**: This multi-step LLM chain (Plan -> Execute -> Synthesize) is a robust pattern. It separates concerns, allowing each component to excel at its specific task. The planner focuses solely on decomposition, the executor focuses on fact extraction from context, and the synthesizer focuses on generating a human-readable, comprehensive final answer. This avoids burdening a single LLM call with a complex, multi-step task, leading to more reliable and accurate results.